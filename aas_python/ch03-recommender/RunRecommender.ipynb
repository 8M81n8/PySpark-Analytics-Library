{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.conf\n",
    "import pyspark.sql\n",
    "SparkConf = pyspark.conf.SparkConf\n",
    "SparkSession = pyspark.sql.SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Intro\") \\\n",
    "            .config('spark.executor.memory', '2g') \\\n",
    "            .config('spark.driver.memory','8g') \\\n",
    "            .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/home/ubuntu/profiledata_06-May-2005/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = datadir\n",
    "rawUserArtistData = spark.read.text(base + \"user_artist_data.txt\")\n",
    "rawArtistData = spark.read.text(base + \"artist_data.txt\")\n",
    "rawArtistAlias = spark.read.text(base + \"artist_alias.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        runRecommender = RunRecommender(spark)\n",
    "        runRecommender.preparation(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "        runRecommender.model(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "        runRecommender.evaluate(rawUserArtistData, rawArtistAlias)\n",
    "        runRecommender.recommend(rawUserArtistData, rawArtistData, rawArtistAlias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value='1000002 1 55')\n",
      "Row(value='1000002 1000006 33')\n",
      "Row(value='1000002 1000007 8')\n",
      "Row(value='1000002 1000009 144')\n",
      "Row(value='1000002 1000010 314')\n"
     ]
    }
   ],
   "source": [
    "for _ in rawUserArtistData.take(5):\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(row):\n",
    "    cols = row.value.split(' ')\n",
    "    user, artist = cols[:2]\n",
    "    return int(user), int(artist)\n",
    "# end def\n",
    "    \n",
    "userArtistDF = rawUserArtistData.rdd.map(fx).toDF([\"user\", \"artist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+-----------+\n",
      "|min(user)|max(user)|min(artist)|max(artist)|\n",
      "+---------+---------+-----------+-----------+\n",
      "|       90|  2443548|          1|   10794401|\n",
      "+---------+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userArtistDF.agg(F.min(\"user\"), F.max(\"user\"), F.min(\"artist\"), F.max(\"artist\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArtistByID(rawArtistData):\n",
    "    def func(row):\n",
    "        try:\n",
    "            (_id, name) = row.value.split('\\t')\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        # end try\n",
    "        if (name.strip() == ''):\n",
    "            return None, None\n",
    "        else:\n",
    "            try:\n",
    "                return int(_id), name.strip()\n",
    "            except:\n",
    "                return None, None\n",
    "            # end try\n",
    "        # end if\n",
    "    # end def\n",
    "    return rawArtistData.rdd.map(func).toDF([\"id\", \"name\"]).na.drop()\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistByID = buildArtistByID(rawArtistData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArtistAlias(rawArtistAlias):\n",
    "    def func(row):\n",
    "        try:\n",
    "            artist, alias = row.value.split('\\t')\n",
    "        except ValueError:\n",
    "            return None, None\n",
    "        # end try\n",
    "        if (artist.strip()==''):\n",
    "            return None, None\n",
    "        else:\n",
    "            return int(artist), int(alias)\n",
    "        # end if\n",
    "    # end def\n",
    "    return dict(rawArtistAlias.rdd.map(func).collect())\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistAlias = buildArtistAlias(rawArtistAlias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "(badID, goodID) = next(iter(artistAlias.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|     id|              name|\n",
      "+-------+------------------+\n",
      "|1109457|             P.O.S|\n",
      "|2097152|DJ Tiesto -  P.O.S|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col('id').isin([badID, goodID])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCounts(rawUserArtistData, bArtistAlias):\n",
    "    def func(row):\n",
    "        try:\n",
    "            userID, artistID, count = map(int, row.value.split(' '))\n",
    "            finalArtistID = bArtistAlias.value.get(artistID, artistID)\n",
    "            return (userID, finalArtistID, count)\n",
    "        except ValueError:\n",
    "            return None, None, None\n",
    "        # end try\n",
    "    # end def\n",
    "    return rawUserArtistData.rdd.map(func).toDF([\"user\", \"artist\", \"count\"]).na.drop()\n",
    "# end def\n",
    "trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS().\\\n",
    "    setSeed(random.randrange(0,10000000)).\\\n",
    "    setImplicitPrefs(True).\\\n",
    "    setRank(10).\\\n",
    "    setRegParam(0.01).\\\n",
    "    setAlpha(1.0).\\\n",
    "    setMaxIter(5).\\\n",
    "    setUserCol(\"user\").\\\n",
    "    setItemCol(\"artist\").\\\n",
    "    setRatingCol(\"count\").\\\n",
    "    setPredictionCol(\"prediction\").\\\n",
    "    fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: bigint, artist: bigint, count: bigint]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                                                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[-0.026651615, 0.7032719, 0.2558946, -0.40509853, 0.11468698, 0.38856968, -0.55922914, 0.61060536, -0.62590265, -0.83069855]                   |\n",
      "|[0.07361874, 0.07843246, 0.17066856, 0.18403913, 0.20880459, -0.21302597, 0.17672752, 0.10975869, -0.17468008, -0.12103835]                    |\n",
      "|[0.0072502233, 0.0022113367, 0.0023733207, -0.0058102906, 0.0015046557, 0.0062324544, -0.0021433341, 0.0028819693, 1.2387775E-4, -1.4924115E-4]|\n",
      "|[-0.09531816, 0.69944614, 0.2610822, 0.15259303, -0.5126287, 0.18788792, -0.3447748, 1.2580154, -1.1473079, -1.4304459]                        |\n",
      "|[-0.274901, 0.4603869, 0.7350271, 0.08513728, 0.7064413, -0.19489343, 0.2773581, 0.8046217, -1.6482328, -1.7964089]                            |\n",
      "|[-0.36708084, -0.14868881, 1.1138638, 3.201273, 0.2818511, -0.99509054, 1.1910527, 2.679362, -1.1990902, 2.6309125]                            |\n",
      "|[-0.1093979, 0.55220264, 0.7407764, 0.39120355, 0.15449893, -0.76928353, 0.67127806, 1.0630081, -0.9349566, -1.1387254]                        |\n",
      "|[0.047258306, 0.03324353, 0.0040977187, 0.073417306, 0.009148026, -0.0445706, -0.05605165, 0.0552819, -0.081686206, -0.0013399193]             |\n",
      "|[1.1977588, 0.42098966, 0.3325627, 0.54154456, -0.26848042, -0.87986386, 0.41790104, -0.35295066, -1.5566497, 1.02832]                         |\n",
      "|[-0.012174995, 1.2687086, 0.18059897, 0.2513606, 0.5928162, -0.500511, 0.8365098, 0.42038846, -0.7785433, -0.5522536]                          |\n",
      "|[0.3153439, 1.7480124, 0.23259863, 0.7630529, 0.8932894, 0.86800647, -0.0641354, 1.9270238, -2.8299854, -1.4270016]                            |\n",
      "|[-0.93408304, 0.9792492, 0.8613709, 1.3956751, 2.6059299, -0.20408456, 0.6886018, 1.2479845, -2.2583823, -0.45940453]                          |\n",
      "|[-0.038962197, 0.7371463, 1.1762052, 1.2918861, 2.0026207, -0.44635394, 0.356682, 0.4567214, -1.7070534, -0.6769842]                           |\n",
      "|[0.71373904, 0.3296372, 0.35736138, 2.0873785, 2.0891104, 9.929629E-4, 0.6698203, 0.9825266, -2.8719485, -0.11899834]                          |\n",
      "|[-0.5582027, -0.16761802, 0.3746909, 0.16972537, 0.7209818, -0.19928654, 0.53983706, 0.9878964, -0.28645748, -0.8337069]                       |\n",
      "|[0.033105463, 0.14563248, 0.49091566, 1.1389949, 1.1569408, -0.27784723, 0.11596768, 0.2450524, -0.73275006, -0.052514523]                     |\n",
      "|[-2.6003237, 1.0896119, 0.20403537, -0.088780165, 0.12678516, -0.071382985, 2.0526342, 3.0920174, -2.8984315, -1.1447736]                      |\n",
      "|[0.0953195, 0.15593767, 0.32068723, 0.46317378, 0.38637266, -0.3329501, 0.51001227, 0.3086444, -0.3629819, -0.16506295]                        |\n",
      "|[0.010968709, 0.021006746, 0.046567015, 0.022409705, 0.048553646, 0.015573932, -0.008851894, -0.02212209, -0.05207392, -0.020303179]           |\n",
      "|[-0.73822874, -0.59270984, 0.6616826, 0.9707593, 1.7120434, -0.16527635, 0.42998344, 1.0269433, -1.3506092, -1.3724014]                        |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.userFactors.select(\"features\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user=1000002, artist=1, count=55),\n",
       " Row(user=1000002, artist=1000006, count=33),\n",
       " Row(user=1000002, artist=1000007, count=8),\n",
       " Row(user=1000002, artist=1000009, count=144),\n",
       " Row(user=1000002, artist=1000010, count=314)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = 2093760\n",
    "\n",
    "existingArtistIDs = trainData.\\\n",
    "    filter(F.col(\"user\") == userID).rdd.\\\n",
    "    map(lambda row: int(row.artist)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistByID = buildArtistByID(rawArtistData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+\n",
      "|     id|           name|\n",
      "+-------+---------------+\n",
      "|   1180|     David Gray|\n",
      "|    378|  Blackalicious|\n",
      "|    813|     Jurassic 5|\n",
      "|1255340|The Saw Doctors|\n",
      "|    942|         Xzibit|\n",
      "+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col(\"id\").isin(existingArtistIDs)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "toRecommend = model.itemFactors.\\\n",
    "        select(F.col(\"id\").alias(\"artist\")).\\\n",
    "        withColumn(\"user\", F.lit(userID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(toRecommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist=30, user=2093760),\n",
       " Row(artist=40, user=2093760),\n",
       " Row(artist=50, user=2093760),\n",
       " Row(artist=70, user=2093760),\n",
       " Row(artist=90, user=2093760)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toRecommend.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method transform in module pyspark.ml.base:\n",
      "\n",
      "transform(dataset, params=None) method of pyspark.ml.recommendation.ALSModel instance\n",
      "    Transforms the input dataset with optional parameters.\n",
      "    \n",
      "    :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      "    :param params: an optional param map that overrides embedded params.\n",
      "    :returns: transformed dataset\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeRecommendations(model, userID, howMany):\n",
    "    toRecommend = model.itemFactors.\\\n",
    "        select(F.col(\"id\").alias(\"artist\")).\\\n",
    "        withColumn(\"user\", F.lit(userID))\n",
    "    ans = model.transform(toRecommend).\\\n",
    "        select([\"artist\", \"prediction\"]).\\\n",
    "        orderBy(F.col(\"prediction\"), ascending=False).\\\n",
    "        limit(howMany)\n",
    "    return ans\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "| artist| prediction|\n",
      "+-------+-----------+\n",
      "|1001819|0.027669843|\n",
      "|   2814|0.027642187|\n",
      "|1300642|0.027621742|\n",
      "|   4605|0.027328338|\n",
      "|1007614|0.027036585|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topRecommendations = makeRecommendations(model, userID, 5)\n",
    "topRecommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendedArtistIDs = topRecommendations.select(\"artist\").rdd.map(lambda row: int(row['artist'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|     id|      name|\n",
      "+-------+----------+\n",
      "|   2814|   50 Cent|\n",
      "|   4605|Snoop Dogg|\n",
      "|1007614|     Jay-Z|\n",
      "|1001819|      2Pac|\n",
      "|1300642|  The Game|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artistByID.filter(F.col(\"id\").isin(recommendedArtistIDs)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, features: array<float>]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.userFactors.unpersist()\n",
    "model.itemFactors.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bArtistAlias = spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "trainData, cvData = allData.randomSplit([0.9, 0.1])\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "\n",
    "allArtistIDs = allData.select(\"artist\").as(IntegerType).distinct().collect()\n",
    "bAllArtistIDs = self.spark.sparkContext.broadcast(allArtistIDs)\n",
    "\n",
    "mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "print(mostListenedAUC)\n",
    "\n",
    "        evaluations =\n",
    "            for (rank         <- Seq(5,    30);\n",
    "                     regParam <- Seq(1.0, 0.0001);\n",
    "                     alpha        <- Seq(1.0, 40.0))\n",
    "            yield {\n",
    "                model = ALS().\n",
    "                    setSeed(Random.nextLong()).\n",
    "                    setImplicitPrefs(true).\n",
    "                    setRank(rank).setRegParam(regParam).\n",
    "                    setAlpha(alpha).setMaxIter(20).\n",
    "                    setUserCol(\"user\").setItemCol(\"artist\").\n",
    "                    setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "                    fit(trainData)\n",
    "\n",
    "                auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "                model.userFactors.unpersist()\n",
    "                model.itemFactors.unpersist()\n",
    "\n",
    "                (auc, (rank, regParam, alpha))\n",
    "            # end def\n",
    "\n",
    "        evaluations.sorted.reverse.foreach(print)\n",
    "\n",
    "        trainData.unpersist()\n",
    "        cvData.unpersist()\n",
    "    # end def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    def recommend(\n",
    "            rawUserArtistData,\n",
    "            rawArtistData,\n",
    "            rawArtistAlias):\n",
    "\n",
    "        bArtistAlias = self.spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "        allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "        model = ALS().\n",
    "            setSeed(Random.nextLong()).\n",
    "            setImplicitPrefs(true).\n",
    "            setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\n",
    "            setUserCol(\"user\").setItemCol(\"artist\").\n",
    "            setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "            fit(allData)\n",
    "        allData.unpersist()\n",
    "\n",
    "        userID = 2093760\n",
    "        topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "        recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "        artistByID = buildArtistByID(rawArtistData)\n",
    "        artistByID.join(self.spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\n",
    "            select(\"name\").show()\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "    # end def\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predictMostListened(train)(allData):\n",
    "        listenCounts = train.groupBy(\"artist\").\n",
    "            agg(sum(\"count\").as(\"prediction\")).\n",
    "            select(\"artist\", \"prediction\")\n",
    "        allData.\n",
    "            join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "            select(\"user\", \"artist\", \"prediction\")\n",
    "    # end def\n",
    "\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def areaUnderCurve(positiveData, bAllArtistIDs, predictFunction):\n",
    "    \n",
    "    # What this actually computes is AUC, per user. The result is actually something\n",
    "    # that might be called \"mean AUC\".\n",
    "\n",
    "    # Take held-out data as the \"positive\".\n",
    "    # Make predictions for each of them, including a numeric score\n",
    "    positivePredictions = predictFunction(positiveData.select([\"user\", \"artist\"])).\n",
    "        withColumnRenamed([\"prediction\", \"positivePrediction\"])\n",
    "\n",
    "    # BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "    # small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "    # Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "    # from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "    negativeData = positiveData.select([\"user\", \"artist\"]).as((IntegerType, IntegerType)).\n",
    "        groupByKey { case (user, _) : user # end def.\n",
    "        flatMapGroups { case (userID, userIDAndPosArtistIDs) :\n",
    "            random = Random()\n",
    "            posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) : artist # end def.toSet\n",
    "            negative = ArrayBuffer[Int]()\n",
    "            allArtistIDs = bAllArtistIDs.value\n",
    "            var i = 0\n",
    "            # Make at most one pass over all artists to avoid an infinite loop.\n",
    "            # Also stop when number of negative equals positive set size\n",
    "            while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "                artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "                # Only add distinct IDs\n",
    "                if (!posItemIDSet.contains(artistID)) {\n",
    "                    negative += artistID\n",
    "                # end def\n",
    "                i += 1\n",
    "            # end def\n",
    "            # Return the set with user ID added back\n",
    "            negative.map(artistID : (userID, artistID))\n",
    "        # end def.toDF(\"user\", \"artist\")\n",
    "\n",
    "    # Make predictions on the rest:\n",
    "    negativePredictions = predictFunction(negativeData).\n",
    "        withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "    # Join positive predictions to negative predictions by user, only.\n",
    "    # This will result in a row for every possible pairing of positive and negative\n",
    "    # predictions within each user.\n",
    "    joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "        select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "    # Count the number of pairs per user\n",
    "    allCounts = joinedPredictions.\n",
    "        groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "        select(\"user\", \"total\")\n",
    "    # Count the number of correctly ordered pairs per user\n",
    "    correctCounts = joinedPredictions.\n",
    "        filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "        groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "        select(\"user\", \"correct\")\n",
    "\n",
    "    # Combine these, compute their ratio, and average over all users\n",
    "    meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "        select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "        agg(mean(\"auc\")).\n",
    "        as[Double].first()\n",
    "\n",
    "    joinedPredictions.unpersist()\n",
    "\n",
    "    meanAUC\n",
    "# end def\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
