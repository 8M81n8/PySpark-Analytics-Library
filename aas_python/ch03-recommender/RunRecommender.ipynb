{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ad1c6d1dc59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Intro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.driver.memory'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'8g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PySpark-Analytics-Library/aas_python/ch03-recommender/venv/lib/python3.5/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "import pyspark.conf\n",
    "import pyspark.sql\n",
    "SparkConf = pyspark.conf.SparkConf\n",
    "SparkSession = pyspark.sql.SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"Intro\") \\\n",
    "            .config('spark.executor.memory', '2g') \\\n",
    "            .config('spark.driver.memory','8g') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.conf\n",
    "import pyspark.sql\n",
    "SparkConf = pyspark.conf.SparkConf\n",
    "SparkSession = pyspark.sql.SparkSession\n",
    "\n",
    "class RunRecommender:\n",
    "\n",
    "    def main(self):\n",
    "        spark = SparkSession.builder().getOrCreate()\n",
    "        # Optional, but may help avoid errors due to long lineage\n",
    "        self.spark.sparkContext.setCheckpointDir(\"hdfs://tmp/\")\n",
    "\n",
    "        base = \"hdfs://user/ds/\"\n",
    "        rawUserArtistData = spark.read.textFile(base + \"user_artist_data.txt\")\n",
    "        rawArtistData = spark.read.textFile(base + \"artist_data.txt\")\n",
    "        rawArtistAlias = spark.read.textFile(base + \"artist_alias.txt\")\n",
    "\n",
    "        runRecommender = RunRecommender(spark)\n",
    "        runRecommender.preparation(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "        runRecommender.model(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "        runRecommender.evaluate(rawUserArtistData, rawArtistAlias)\n",
    "        runRecommender.recommend(rawUserArtistData, rawArtistData, rawArtistAlias)\n",
    "    # end def\n",
    "\n",
    "# end class\n",
    "\n",
    "class RunRecommender:\n",
    "\n",
    "    def __init__(spark):\n",
    "        self.spark = spark\n",
    "    # end def\n",
    "\n",
    "    def preparation(\n",
    "            rawUserArtistData,\n",
    "            rawArtistData,\n",
    "            rawArtistAlias):\n",
    "\n",
    "        rawUserArtistData.take(5).foreach(print)\n",
    "\n",
    "        userArtistDF = rawUserArtistData.map { line :\n",
    "            Array(user, artist, _*) = line.split(' ')\n",
    "            (user.toInt, artist.toInt)\n",
    "        # end def.toDF(\"user\", \"artist\")\n",
    "\n",
    "        userArtistDF.agg(min(\"user\"), max(\"user\"), min(\"artist\"), max(\"artist\")).show()\n",
    "\n",
    "        artistByID = buildArtistByID(rawArtistData)\n",
    "        artistAlias = buildArtistAlias(rawArtistAlias)\n",
    "\n",
    "        (badID, goodID) = artistAlias.head\n",
    "        artistByID.filter($\"id\" isin (badID, goodID)).show()\n",
    "    # end def\n",
    "\n",
    "    def model(rawUserArtistData, rawArtistData, rawArtistAlias):\n",
    "\n",
    "        bArtistAlias = self.spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "        trainData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "\n",
    "        model = ALS().\n",
    "            setSeed(Random.nextLong()).\n",
    "            setImplicitPrefs(true).\n",
    "            setRank(10).\n",
    "            setRegParam(0.01).\n",
    "            setAlpha(1.0).\n",
    "            setMaxIter(5).\n",
    "            setUserCol(\"user\").\n",
    "            setItemCol(\"artist\").\n",
    "            setRatingCol(\"count\").\n",
    "            setPredictionCol(\"prediction\").\n",
    "            fit(trainData)\n",
    "\n",
    "        trainData.unpersist()\n",
    "\n",
    "        model.userFactors.select(\"features\").show(truncate = false)\n",
    "\n",
    "        userID = 2093760\n",
    "\n",
    "        existingArtistIDs = trainData.\n",
    "            filter($\"user\" === userID).\n",
    "            select(\"artist\").as[Int].collect()\n",
    "\n",
    "        artistByID = buildArtistByID(rawArtistData)\n",
    "\n",
    "        artistByID.filter($\"id\" isin (existingArtistIDs:_*)).show()\n",
    "\n",
    "        topRecommendations = makeRecommendations(model, userID, 5)\n",
    "        topRecommendations.show()\n",
    "\n",
    "        recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "\n",
    "        artistByID.filter($\"id\" isin (recommendedArtistIDs:_*)).show()\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "    # end def\n",
    "\n",
    "    def evaluate(\n",
    "            rawUserArtistData,\n",
    "            rawArtistAlias):\n",
    "\n",
    "        bArtistAlias = self.spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "\n",
    "        allData = buildCounts(rawUserArtistData, bArtistAlias)\n",
    "        Array(trainData, cvData) = allData.randomSplit(Array(0.9, 0.1))\n",
    "        trainData.cache()\n",
    "        cvData.cache()\n",
    "\n",
    "        allArtistIDs = allData.select(\"artist\").as[Int].distinct().collect()\n",
    "        bAllArtistIDs = self.spark.sparkContext.broadcast(allArtistIDs)\n",
    "\n",
    "        mostListenedAUC = areaUnderCurve(cvData, bAllArtistIDs, predictMostListened(trainData))\n",
    "        print(mostListenedAUC)\n",
    "\n",
    "        evaluations =\n",
    "            for (rank         <- Seq(5,    30);\n",
    "                     regParam <- Seq(1.0, 0.0001);\n",
    "                     alpha        <- Seq(1.0, 40.0))\n",
    "            yield {\n",
    "                model = ALS().\n",
    "                    setSeed(Random.nextLong()).\n",
    "                    setImplicitPrefs(true).\n",
    "                    setRank(rank).setRegParam(regParam).\n",
    "                    setAlpha(alpha).setMaxIter(20).\n",
    "                    setUserCol(\"user\").setItemCol(\"artist\").\n",
    "                    setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "                    fit(trainData)\n",
    "\n",
    "                auc = areaUnderCurve(cvData, bAllArtistIDs, model.transform)\n",
    "\n",
    "                model.userFactors.unpersist()\n",
    "                model.itemFactors.unpersist()\n",
    "\n",
    "                (auc, (rank, regParam, alpha))\n",
    "            # end def\n",
    "\n",
    "        evaluations.sorted.reverse.foreach(print)\n",
    "\n",
    "        trainData.unpersist()\n",
    "        cvData.unpersist()\n",
    "    # end def\n",
    "\n",
    "    def recommend(\n",
    "            rawUserArtistData,\n",
    "            rawArtistData,\n",
    "            rawArtistAlias):\n",
    "\n",
    "        bArtistAlias = self.spark.sparkContext.broadcast(buildArtistAlias(rawArtistAlias))\n",
    "        allData = buildCounts(rawUserArtistData, bArtistAlias).cache()\n",
    "        model = ALS().\n",
    "            setSeed(Random.nextLong()).\n",
    "            setImplicitPrefs(true).\n",
    "            setRank(10).setRegParam(1.0).setAlpha(40.0).setMaxIter(20).\n",
    "            setUserCol(\"user\").setItemCol(\"artist\").\n",
    "            setRatingCol(\"count\").setPredictionCol(\"prediction\").\n",
    "            fit(allData)\n",
    "        allData.unpersist()\n",
    "\n",
    "        userID = 2093760\n",
    "        topRecommendations = makeRecommendations(model, userID, 5)\n",
    "\n",
    "        recommendedArtistIDs = topRecommendations.select(\"artist\").as[Int].collect()\n",
    "        artistByID = buildArtistByID(rawArtistData)\n",
    "        artistByID.join(self.spark.createDataset(recommendedArtistIDs).toDF(\"id\"), \"id\").\n",
    "            select(\"name\").show()\n",
    "\n",
    "        model.userFactors.unpersist()\n",
    "        model.itemFactors.unpersist()\n",
    "    # end def\n",
    "\n",
    "    def buildArtistByID(rawArtistData):\n",
    "        rawArtistData.flatMap { line :\n",
    "            (id, name) = line.span(_ != '\\t')\n",
    "            if (name.isEmpty) {\n",
    "                None\n",
    "            # end def else {\n",
    "                try {\n",
    "                    Some((id.toInt, name.trim))\n",
    "                # end def catch {\n",
    "                    case _: NumberFormatException : None\n",
    "                # end def\n",
    "            # end def\n",
    "        # end def.toDF(\"id\", \"name\")\n",
    "    # end def\n",
    "\n",
    "    def buildArtistAlias(rawArtistAlias): Map[Int,Int]:\n",
    "        rawArtistAlias.flatMap { line :\n",
    "            Array(artist, alias) = line.split('\\t')\n",
    "            if (artist.isEmpty) {\n",
    "                None\n",
    "            # end def else {\n",
    "                Some((artist.toInt, alias.toInt))\n",
    "            # end def\n",
    "        # end def.collect().toMap\n",
    "    # end def\n",
    "\n",
    "    def buildCounts(\n",
    "            rawUserArtistData,\n",
    "            bArtistAlias: Broadcast[Map[Int,Int]]):\n",
    "        rawUserArtistData.map { line :\n",
    "            Array(userID, artistID, count) = line.split(' ').map(_.toInt)\n",
    "            finalArtistID = bArtistAlias.value.getOrElse(artistID, artistID)\n",
    "            (userID, finalArtistID, count)\n",
    "        # end def.toDF(\"user\", \"artist\", \"count\")\n",
    "    # end def\n",
    "\n",
    "    def makeRecommendations(model: ALSModel, userID, howMany):\n",
    "        toRecommend = model.itemFactors.\n",
    "            select($\"id\".as(\"artist\")).\n",
    "            withColumn(\"user\", lit(userID))\n",
    "        model.transform(toRecommend).\n",
    "            select(\"artist\", \"prediction\").\n",
    "            orderBy($\"prediction\".desc).\n",
    "            limit(howMany)\n",
    "    # end def\n",
    "\n",
    "    def areaUnderCurve(\n",
    "            positiveData,\n",
    "            bAllArtistIDs: Broadcast[Array[Int]],\n",
    "            predictFunction: (DataFrame : DataFrame)):\n",
    "\n",
    "        # What this actually computes is AUC, per user. The result is actually something\n",
    "        # that might be called \"mean AUC\".\n",
    "\n",
    "        # Take held-out data as the \"positive\".\n",
    "        # Make predictions for each of them, including a numeric score\n",
    "        positivePredictions = predictFunction(positiveData.select(\"user\", \"artist\")).\n",
    "            withColumnRenamed(\"prediction\", \"positivePrediction\")\n",
    "\n",
    "        # BinaryClassificationMetrics.areaUnderROC is not used here since there are really lots of\n",
    "        # small AUC problems, and it would be inefficient, when a direct computation is available.\n",
    "\n",
    "        # Create a set of \"negative\" products for each user. These are randomly chosen\n",
    "        # from among all of the other artists, excluding those that are \"positive\" for the user.\n",
    "        negativeData = positiveData.select(\"user\", \"artist\").as[(Int,Int)].\n",
    "            groupByKey { case (user, _) : user # end def.\n",
    "            flatMapGroups { case (userID, userIDAndPosArtistIDs) :\n",
    "                random = Random()\n",
    "                posItemIDSet = userIDAndPosArtistIDs.map { case (_, artist) : artist # end def.toSet\n",
    "                negative = ArrayBuffer[Int]()\n",
    "                allArtistIDs = bAllArtistIDs.value\n",
    "                var i = 0\n",
    "                # Make at most one pass over all artists to avoid an infinite loop.\n",
    "                # Also stop when number of negative equals positive set size\n",
    "                while (i < allArtistIDs.length && negative.size < posItemIDSet.size) {\n",
    "                    artistID = allArtistIDs(random.nextInt(allArtistIDs.length))\n",
    "                    # Only add distinct IDs\n",
    "                    if (!posItemIDSet.contains(artistID)) {\n",
    "                        negative += artistID\n",
    "                    # end def\n",
    "                    i += 1\n",
    "                # end def\n",
    "                # Return the set with user ID added back\n",
    "                negative.map(artistID : (userID, artistID))\n",
    "            # end def.toDF(\"user\", \"artist\")\n",
    "\n",
    "        # Make predictions on the rest:\n",
    "        negativePredictions = predictFunction(negativeData).\n",
    "            withColumnRenamed(\"prediction\", \"negativePrediction\")\n",
    "\n",
    "        # Join positive predictions to negative predictions by user, only.\n",
    "        # This will result in a row for every possible pairing of positive and negative\n",
    "        # predictions within each user.\n",
    "        joinedPredictions = positivePredictions.join(negativePredictions, \"user\").\n",
    "            select(\"user\", \"positivePrediction\", \"negativePrediction\").cache()\n",
    "\n",
    "        # Count the number of pairs per user\n",
    "        allCounts = joinedPredictions.\n",
    "            groupBy(\"user\").agg(count(lit(\"1\")).as(\"total\")).\n",
    "            select(\"user\", \"total\")\n",
    "        # Count the number of correctly ordered pairs per user\n",
    "        correctCounts = joinedPredictions.\n",
    "            filter($\"positivePrediction\" > $\"negativePrediction\").\n",
    "            groupBy(\"user\").agg(count(\"user\").as(\"correct\")).\n",
    "            select(\"user\", \"correct\")\n",
    "\n",
    "        # Combine these, compute their ratio, and average over all users\n",
    "        meanAUC = allCounts.join(correctCounts, Seq(\"user\"), \"left_outer\").\n",
    "            select($\"user\", (coalesce($\"correct\", lit(0)) / $\"total\").as(\"auc\")).\n",
    "            agg(mean(\"auc\")).\n",
    "            as[Double].first()\n",
    "\n",
    "        joinedPredictions.unpersist()\n",
    "\n",
    "        meanAUC\n",
    "    # end def\n",
    "\n",
    "    def predictMostListened(train)(allData):\n",
    "        listenCounts = train.groupBy(\"artist\").\n",
    "            agg(sum(\"count\").as(\"prediction\")).\n",
    "            select(\"artist\", \"prediction\")\n",
    "        allData.\n",
    "            join(listenCounts, Seq(\"artist\"), \"left_outer\").\n",
    "            select(\"user\", \"artist\", \"prediction\")\n",
    "    # end def\n",
    "\n",
    "# end class\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = RunRecommender()\n",
    "    app.main()\n",
    "# end if"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
